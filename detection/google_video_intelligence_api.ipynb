{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-videointelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a5b1da265c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m                       sep=' | ')\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m \u001b[0mdetect_logo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nike_movie2.mp4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-5a5b1da265c0>\u001b[0m in \u001b[0;36mdetect_logo\u001b[1;34m(local_file_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideointelligence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoIntelligenceServiceClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0minput_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvideointelligence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLOGO_RECOGNITION\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from google.cloud import videointelligence\n",
    "import cv2\n",
    "\n",
    "\n",
    "def detect_logo(local_file_path):\n",
    "    \"\"\"Performs asynchronous video annotation for logo recognition on a local file.\"\"\"\n",
    "\n",
    "    client = videointelligence.VideoIntelligenceServiceClient()\n",
    "\n",
    "    with io.open(local_file_path, \"rb\") as f:\n",
    "        input_content = f.read()\n",
    "    features = [videointelligence.enums.Feature.LOGO_RECOGNITION]\n",
    "\n",
    "    operation = client.annotate_video(input_content=input_content, features=features)\n",
    "\n",
    "    print(u\"Waiting for operation to complete...\")\n",
    "    response = operation.result()\n",
    "#     print_detected_logos(response)\n",
    "    # Get the first response, since we sent only one video.\n",
    "    annotation_result = response.annotation_results[0]\n",
    "\n",
    "    # Annotations for list of logos detected, tracked and recognized in video.\n",
    "    for logo_recognition_annotation in annotation_result.logo_recognition_annotations:\n",
    "        entity = logo_recognition_annotation.entity\n",
    "#         print_logo_frames(response, entity.entity_id)\n",
    "        # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\n",
    "        # Search API](https://developers.google.com/knowledge-graph/).\n",
    "        print(u\"Entity Id : {}\".format(entity.entity_id))\n",
    "\n",
    "        print(u\"Description : {}\".format(entity.description))\n",
    "\n",
    "        # All logo tracks where the recognized logo appears. Each track corresponds\n",
    "        # to one logo instance appearing in consecutive frames.\n",
    "        for track in logo_recognition_annotation.tracks:\n",
    "            # Video segment of a track.\n",
    "            print(\n",
    "                u\"\\n\\tStart Time Offset : {}.{}\".format(\n",
    "                    track.segment.start_time_offset.seconds,\n",
    "                    track.segment.start_time_offset.nanos,\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                u\"\\tEnd Time Offset : {}.{}\".format(\n",
    "                    track.segment.end_time_offset.seconds,\n",
    "                    track.segment.end_time_offset.nanos,\n",
    "                )\n",
    "            )\n",
    "            print(u\"\\tConfidence : {}\".format(track.confidence))\n",
    "\n",
    "            # The object with timestamp and attributes per frame in the track.\n",
    "            for timestamped_object in track.timestamped_objects:\n",
    "\n",
    "                # Normalized Bounding box in a frame, where the object is located.\n",
    "                normalized_bounding_box = timestamped_object.normalized_bounding_box\n",
    "                print(u\"\\n\\t\\tLeft : {}\".format(normalized_bounding_box.left))\n",
    "                print(u\"\\t\\tTop : {}\".format(normalized_bounding_box.top))\n",
    "                print(u\"\\t\\tRight : {}\".format(normalized_bounding_box.right))\n",
    "                print(u\"\\t\\tBottom : {}\".format(normalized_bounding_box.bottom))\n",
    "\n",
    "                # Optional. The attributes of the object in the bounding box.\n",
    "                for attribute in timestamped_object.attributes:\n",
    "                    print(u\"\\n\\t\\t\\tName : {}\".format(attribute.name))\n",
    "                    print(u\"\\t\\t\\tConfidence : {}\".format(attribute.confidence))\n",
    "                    print(u\"\\t\\t\\tValue : {}\".format(attribute.value))\n",
    "\n",
    "            # Optional. Attributes in the track level.\n",
    "            for track_attribute in track.attributes:\n",
    "                print(u\"\\n\\t\\tName : {}\".format(track_attribute.name))\n",
    "                print(u\"\\t\\tConfidence : {}\".format(track_attribute.confidence))\n",
    "                print(u\"\\t\\tValue : {}\".format(track_attribute.value))\n",
    "\n",
    "        # All video segments where the recognized logo appears. There might be\n",
    "        # multiple instances of the same logo class appearing in one VideoSegment.\n",
    "        for segment in logo_recognition_annotation.segments:\n",
    "            print(\n",
    "                u\"\\n\\tStart Time Offset : {}.{}\".format(\n",
    "                    segment.start_time_offset.seconds, segment.start_time_offset.nanos,\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                u\"\\tEnd Time Offset : {}.{}\".format(\n",
    "                    segment.end_time_offset.seconds, segment.end_time_offset.nanos,\n",
    "                )\n",
    "            )\n",
    "\n",
    "def print_detected_logos(response):\n",
    "    # First result only, as a single video is processed\n",
    "    annotations = response.annotation_results[0].logo_recognition_annotations\n",
    "\n",
    "    print(f' Detected logos: {len(annotations)} '.center(80, '-'))\n",
    "    for annotation in annotations:\n",
    "        entity = annotation.entity\n",
    "        entity_id = entity.entity_id\n",
    "        description = entity.description\n",
    "        for track in annotation.tracks:\n",
    "            confidence = track.confidence\n",
    "            start_ms = track.segment.start_time_offset.ToMilliseconds()\n",
    "            end_ms = track.segment.end_time_offset.ToMilliseconds()\n",
    "            logo_frames = len(track.timestamped_objects)\n",
    "            print(f'{confidence:4.0%}',\n",
    "                  f'{start_ms:>7,}',\n",
    "                  f'{end_ms:>7,}',\n",
    "                  f'{logo_frames:>3} fr.',\n",
    "                  f'{entity_id:<15}',\n",
    "                  f'{description}',\n",
    "                  sep=' | ')\n",
    "            \n",
    "def print_logo_frames(response, entity_id):\n",
    "    def keep_annotation(annotation):\n",
    "        return annotation.entity.entity_id == entity_id\n",
    "\n",
    "    # First result only, as a single video is processed\n",
    "    annotations = response.annotation_results[0].logo_recognition_annotations\n",
    "    annotations = [a for a in annotations if keep_annotation(a)]\n",
    "    for annotation in annotations:\n",
    "        description = annotation.entity.description\n",
    "        for track in annotation.tracks:\n",
    "            confidence = track.confidence\n",
    "            print(f' {description},'\n",
    "                  f' confidence: {confidence:.0%},'\n",
    "                  f' frames: {len(track.timestamped_objects)} '\n",
    "                  .center(80, '-'))\n",
    "            for timestamped_object in track.timestamped_objects:\n",
    "                frame_ms = timestamped_object.time_offset.ToMilliseconds()\n",
    "                box = timestamped_object.normalized_bounding_box\n",
    "                print(f'{frame_ms:>7,}',\n",
    "                      f'({box.left:.5f}, {box.top:.5f})',\n",
    "                      f'({box.right:.5f}, {box.bottom:.5f})',\n",
    "                      sep=' | ')\n",
    "                \n",
    "detect_logo('nike_movie2.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video():\n",
    "    capture = cv2.VideoCapture(\"nike_movie2.mp4\")\n",
    "    while True:\n",
    "        if (capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "            capture.open(\"nike_movie2.mp4\")\n",
    "\n",
    "        ret, frame = capture.read()\n",
    "        cv2.imshow(\"VideoFrame\", frame)\n",
    "\n",
    "        if cv2.waitKey(33) > 0: break\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "CAM_ID = 0\n",
    "def capture(camid = CAM_ID):\n",
    "    cam = cv2.VideoCapture(camid)\n",
    "    if cam.isOpened() == False:\n",
    "        print ('cant open the cam (%d)' % camid)\n",
    "        return None\n",
    "\n",
    "    ret, frame = cam.read()\n",
    "    if frame is None:\n",
    "        print ('frame is not exist')\n",
    "        return None\n",
    "    \n",
    "    # png로 압축 없이 영상 저장 \n",
    "    cv2.imwrite('messigray.png',frame, params=[cv2.IMWRITE_PNG_COMPRESSION,0])\n",
    "    cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
